{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqFXCHPhXjo8"
      },
      "source": [
        "<h2>CS 3780/5780 Creative Project: </h2>\n",
        "<h3>Emotion Classification of Natural Language</h3>\n",
        "\n",
        "Names and NetIDs for your group members:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJE9a_I8Xjo9"
      },
      "source": [
        "<h3>Introduction:</h3>\n",
        "\n",
        "<p> The creative project is about conducting a real-world machine learning project on your own, with everything that is involved. Unlike in the programming projects 1-5, where we gave you all the scaffolding and you just filled in the blanks, you now start from scratch. The past programming projects provide templates for how to do this (and you can reuse part of your code if you wish), and the lectures provide some of the methods you can use. So, this creative project brings realism to how you will use machine learning in the real world.  </p>\n",
        "\n",
        "The task you will work on is classifying texts to human emotions. Through words, humans express feelings, articulate thoughts, and communicate our deepest needs and desires. Language helps us interpret the nuances of joy, sadness, anger, and love, allowing us to connect with others on a deeper level. Are you able to train an ML model that recognizes the human emotions expressed in a piece of text? <b>Please read the project description PDF file carefully and follow the instructions there. Also make sure you write your code and answers to all the questions in this Jupyter Notebook </b> </p>\n",
        "<p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdXhJIIaXjo-"
      },
      "source": [
        "<h2>Part 0: Basics</h2><p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSFo0TKaXjo-"
      },
      "source": [
        "<h3>0.1 Import:</h3><p>\n",
        "Please import necessary packages to use. Note that learning and using packages are recommended but not required for this project. Some official tutorial for suggested packacges includes:\n",
        "    \n",
        "https://scikit-learn.org/stable/tutorial/basic/tutorial.html\n",
        "    \n",
        "https://pytorch.org/tutorials/\n",
        "    \n",
        "https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html\n",
        "<p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "execution": {
          "iopub.execute_input": "2024-12-09T15:12:09.391215Z",
          "iopub.status.busy": "2024-12-09T15:12:09.390742Z"
        },
        "id": "Z8BL_R9RXjo-",
        "outputId": "a34fab01-32d8-4172-b293-0f274b51bb47",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.15.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "# !pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "SvKvna-oXjo_",
        "outputId": "7a793d9e-d32b-475c-b531-757ab6e7eaea",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# !python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "4c3ZNHxXXjpA",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import spacy\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import Trainer, TrainingArguments\n",
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UU6TrGOXjpA"
      },
      "source": [
        "<h3>0.2 Accuracy and Mean Squared Error:</h3><p>\n",
        "To measure your performance in the Kaggle Competition, we are using accuracy. As a recap, accuracy is the percent of labels you predict correctly. To measure this, you can use library functions from sklearn. A simple example is shown below.\n",
        "<p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "xGOn9Q3MXjpA",
        "outputId": "7e69b87a-d478-43b1-b6cb-0dea34fd41bf",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.42857142857142855"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "y_pred = [3, 2, 1, 0, 1, 2, 3]\n",
        "y_true = [0, 1, 2, 3, 1, 2, 3]\n",
        "accuracy_score(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1k6_AQ8XjpA"
      },
      "source": [
        "<h2>Part 1: Basic</h2><p>\n",
        "Note that your code should be commented well and in part 1.4 you can refer to your comments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pJ1yUV0XjpA"
      },
      "source": [
        "<h3>1.1 Load and preprocess the dataset:</h3><p>\n",
        "We provide how to load the data on Kaggle's Notebook.\n",
        "<p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "tYR6B0fLXjpA",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#train = pd.read_csv(\"/kaggle/input/cs-3780-5780-how-do-you-feel/train.csv\")\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "train_text = train[\"text\"]\n",
        "train_label = train[\"label\"]\n",
        "\n",
        "#test = pd.read_csv(\"/kaggle/input/cs-3780-5780-how-do-you-feel/test.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "test_id = test[\"id\"]\n",
        "test_text = test[\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "N3fLn5XwXjpB",
        "outputId": "71afa0c8-3a4e-4b51-f093-c486cd9a39dd",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"train\",\n  \"rows\": 10000,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9936,\n        \"samples\": [\n          \"i don t know what i ve done to curry favor with this particular new dancer but the feeling of being lauded was more pleasant than taxing for a change\",\n          \"i remember him feeling discouraged\",\n          \"i am able to feel incomparability of life most precious resources to your affectionate touch that can bring warmth and awaken my cautious yet feeble heart together our bonds is able to release the troubles that bother us they will steadily sink to darkest depths of the ocean\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 0,\n        \"max\": 27,\n        \"num_unique_values\": 28,\n        \"samples\": [\n          17,\n          0,\n          16\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "train"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-b345403e-e47a-4be8-bcaf-906e793dd536\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i interact with on a daily basis either in rea...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Stranger than fiction. Can't even begin to com...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i sit here with the aftermath feeling so damn ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Great job! Hats off to you.</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i hate you threads posted by people just whini...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b345403e-e47a-4be8-bcaf-906e793dd536')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b345403e-e47a-4be8-bcaf-906e793dd536 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b345403e-e47a-4be8-bcaf-906e793dd536');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d286f807-5f76-40bd-8908-f2e5d2005a51\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d286f807-5f76-40bd-8908-f2e5d2005a51')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d286f807-5f76-40bd-8908-f2e5d2005a51 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                text  label\n",
              "0  i interact with on a daily basis either in rea...      1\n",
              "1  Stranger than fiction. Can't even begin to com...      1\n",
              "2  i sit here with the aftermath feeling so damn ...      1\n",
              "3                        Great job! Hats off to you.     25\n",
              "4  i hate you threads posted by people just whini...      9"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Make sure you comment your code clearly and you may refer to these comments in the part 1.4\n",
        "# TODO\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ubTT4VQXjpB"
      },
      "source": [
        "<h3>1.2 Use At Least Two Training Algorithms from class:</h3><p>\n",
        "You need to use at least two training algorithms from class. You can use your code from previous projects or any packages you imported in part 0.1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiNRn2f-IrGi"
      },
      "source": [
        "**Bag of Words Vectorization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8h7_c16OXjpB",
        "outputId": "5ed96fea-e34c-4edf-cfde-9e072876c09b",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        }
      ],
      "source": [
        "#import spacy, a library that will aid in getting rid of unuseful features\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "#extract all words from the text and add to cleaned list\n",
        "cleaned = []\n",
        "for text in train['text']:\n",
        "    doc = nlp(text)\n",
        "    #filter out unimportant features, such as \"and\".\n",
        "    #word.lemma_ converts verbs like running into run.\n",
        "    filtered = [word.lemma_ for word in doc if not word.is_stop]\n",
        "    cleaned.append(filtered)\n",
        "\n",
        "#create a set to hold all unique words\n",
        "vocab = {}\n",
        "\n",
        "#add unique words to vocab and add index to value for indexing later.\n",
        "for sentence in cleaned:\n",
        "    for word in sentence:\n",
        "        if word not in vocab:\n",
        "            vocab[word] = len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "9c0LVTogXjpB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#transform text into vector by creating tensor representing each word in the vocab\n",
        "#then counting the number of occurences of the vocab words in each sentence.\n",
        "def vectorize(text, vocab):\n",
        "  vector = np.zeros(len(vocab))\n",
        "  for word in text.lower().split():\n",
        "    if word in vocab:\n",
        "      vector[vocab[word]] += 1\n",
        "  return vector\n",
        "\n",
        "#vectors is the list of all sentences transformed into vectors\n",
        "#labels is the list of all labels corresponding to the vectors\n",
        "vectors = []\n",
        "labels = []\n",
        "\n",
        "#simultaneously iterate over the sentences and labels and apply vectorization\n",
        "#to each sentence and populate vectors and labels list\n",
        "for text, label in zip(train['text'], train['label']):\n",
        "    vectors.append(vectorize(text, vocab))\n",
        "    labels.append(label)\n",
        "\n",
        "#convert both vectors and labels into tensors and convert to float32 for pytorch\n",
        "vectors = torch.tensor(np.array(vectors), dtype=torch.float32)\n",
        "labels = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "#create bag of words model using a deep neural net in pytorch\n",
        "class BoWModel(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, output_size):\n",
        "        super(BoWModel, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(vocab_size, hidden_size)\n",
        "        self.fc2 = torch.nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = torch.nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        #added batchnorm layer to make training faster and more stable\n",
        "        self.bn1 = torch.nn.BatchNorm1d(hidden_size)\n",
        "\n",
        "        #added dropout since there is large amounts of overfitting\n",
        "        self.dropout = torch.nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #use leaky relu so there are no vanishing gradients\n",
        "        leaky = torch.nn.LeakyReLU(negative_slope=0.1)\n",
        "        x = leaky(self.bn1(self.fc1(x)))\n",
        "        #apply dropout to output from previous step\n",
        "        x = self.dropout(x)\n",
        "        x = leaky(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "#create model with correct input/output dimensions. hidden size of 100 is arbitrary\n",
        "model = BoWModel(len(vocab), 100, 28)\n",
        "\n",
        "#set loss to cross entropy since we are working with discrete outputs\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "#set optimizer to adam since it is the most current efficient and effectice optimizer\n",
        "#add regulariztion (weight_decay) to penzalize large weights and reduce overfitting\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_XARkHRXjpB"
      },
      "source": [
        "<h3>1.3 Training, Validation and Model Selection:</h3><p>\n",
        "You need to split your data to a training set and validation set or performing a cross-validation for model selection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-HB5Q8kZXjpB",
        "outputId": "2b224ca3-16c1-40e0-a213-970fc9cdea84",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "  Train Loss: 3.3998\n",
            "  Validation Loss: 3.3438\n",
            "  Validation Accuracy: 0.70%\n",
            "Epoch 26/200\n",
            "  Train Loss: 3.0209\n",
            "  Validation Loss: 3.3145\n",
            "  Validation Accuracy: 2.35%\n",
            "Epoch 51/200\n",
            "  Train Loss: 2.6353\n",
            "  Validation Loss: 3.1966\n",
            "  Validation Accuracy: 41.90%\n",
            "Epoch 76/200\n",
            "  Train Loss: 2.2344\n",
            "  Validation Loss: 2.7724\n",
            "  Validation Accuracy: 58.80%\n",
            "Epoch 101/200\n",
            "  Train Loss: 1.8446\n",
            "  Validation Loss: 2.2464\n",
            "  Validation Accuracy: 63.05%\n",
            "Epoch 126/200\n",
            "  Train Loss: 1.5118\n",
            "  Validation Loss: 1.9260\n",
            "  Validation Accuracy: 65.45%\n",
            "Epoch 151/200\n",
            "  Train Loss: 1.2496\n",
            "  Validation Loss: 1.6988\n",
            "  Validation Accuracy: 66.65%\n",
            "Epoch 176/200\n",
            "  Train Loss: 1.0507\n",
            "  Validation Loss: 1.5349\n",
            "  Validation Accuracy: 67.95%\n",
            "Epoch 200/200\n",
            "  Train Loss: 0.9174\n",
            "  Validation Loss: 1.4326\n",
            "  Validation Accuracy: 68.35%\n"
          ]
        }
      ],
      "source": [
        "# Make sure you comment your code clearly and you may refer to these comments in the part 1.4\n",
        "# TODO\n",
        "\n",
        "#create integer on where to split train/validation set at 80%\n",
        "split = int(len(vectors) * 0.8)\n",
        "\n",
        "#create train and validation lists for vector embeddings and labels\n",
        "train_vectors = vectors[:split]\n",
        "train_labels = labels[:split]\n",
        "valid_vectors = (vectors[split:])\n",
        "valid_labels = (labels[split:])\n",
        "\n",
        "#training loop for BoW and Nerual Net model\n",
        "num_epochs = 200\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    train_preds = model(train_vectors)\n",
        "    train_loss = loss(train_preds, train_labels.long())\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    #get validation set predicitions (logits and non logits)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        valid_preds = model(valid_vectors)\n",
        "        valid_loss = loss(valid_preds, valid_labels.long())\n",
        "        valid_preds_arg = torch.argmax(valid_preds, dim=1)\n",
        "        valid_accuracy = (valid_preds_arg == valid_labels.long()).sum().item() / valid_labels.size(0)\n",
        "\n",
        "    #print train loss, validation loss, validation accuracy every 25 epochs\n",
        "    if epoch % 25 == 0 or epoch == num_epochs - 1:\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "        print(f\"  Train Loss: {train_loss.item():.4f}\")\n",
        "        print(f\"  Validation Loss: {valid_loss.item():.4f}\")\n",
        "        print(f\"  Validation Accuracy: {valid_accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "wSU4mdYJXjpC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#create list to hold vectors in test set, populate list, convert list into tensor of float32\n",
        "test_vectors = []\n",
        "for text in test['text']:\n",
        "    test_vectors.append(vectorize(text, vocab))\n",
        "test_vectors = torch.tensor(np.array(test_vectors), dtype=torch.float32)\n",
        "\n",
        "#get predictions for test set and get non_logit output\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_preds = model(test_vectors)\n",
        "    test_preds_arg = torch.argmax(test_preds, dim=1).numpy()\n",
        "\n",
        "#create pandas df in correct format\n",
        "predictions_df = pd.DataFrame({\n",
        "    'id': test['id'],\n",
        "    'label': test_preds_arg\n",
        "})\n",
        "\n",
        "#write predictions to csv\n",
        "predictions_df.to_csv('submission.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9flOOeImf5Mb",
        "outputId": "c30ec80d-028f-4d00-945a-c7f2b0af3a5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8000, 10035])\n"
          ]
        }
      ],
      "source": [
        "print(train_vectors.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAMPfs7tYjsD"
      },
      "source": [
        "Now We will use a gradient boosted decision tree for our second model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "n7UlOlMeYo5p",
        "outputId": "87ac7ff8-31ce-40a2-f074-518886e64298"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0]\tvalidation_0-mlogloss:3.05115\tvalidation_1-mlogloss:3.06361\n",
            "[1]\tvalidation_0-mlogloss:2.86536\tvalidation_1-mlogloss:2.88056\n",
            "[2]\tvalidation_0-mlogloss:2.72320\tvalidation_1-mlogloss:2.74551\n",
            "[3]\tvalidation_0-mlogloss:2.60764\tvalidation_1-mlogloss:2.63338\n",
            "[4]\tvalidation_0-mlogloss:2.51228\tvalidation_1-mlogloss:2.54337\n",
            "[5]\tvalidation_0-mlogloss:2.42938\tvalidation_1-mlogloss:2.46322\n",
            "[6]\tvalidation_0-mlogloss:2.35822\tvalidation_1-mlogloss:2.39629\n",
            "[7]\tvalidation_0-mlogloss:2.29524\tvalidation_1-mlogloss:2.33612\n",
            "[8]\tvalidation_0-mlogloss:2.23948\tvalidation_1-mlogloss:2.28323\n",
            "[9]\tvalidation_0-mlogloss:2.18893\tvalidation_1-mlogloss:2.23567\n",
            "[10]\tvalidation_0-mlogloss:2.14308\tvalidation_1-mlogloss:2.19147\n",
            "[11]\tvalidation_0-mlogloss:2.10106\tvalidation_1-mlogloss:2.14954\n",
            "[12]\tvalidation_0-mlogloss:2.06260\tvalidation_1-mlogloss:2.11459\n",
            "[13]\tvalidation_0-mlogloss:2.02728\tvalidation_1-mlogloss:2.08037\n",
            "[14]\tvalidation_0-mlogloss:1.99463\tvalidation_1-mlogloss:2.05003\n",
            "[15]\tvalidation_0-mlogloss:1.96439\tvalidation_1-mlogloss:2.02123\n",
            "[16]\tvalidation_0-mlogloss:1.93619\tvalidation_1-mlogloss:1.99489\n",
            "[17]\tvalidation_0-mlogloss:1.91001\tvalidation_1-mlogloss:1.97040\n",
            "[18]\tvalidation_0-mlogloss:1.88562\tvalidation_1-mlogloss:1.94669\n",
            "[19]\tvalidation_0-mlogloss:1.86222\tvalidation_1-mlogloss:1.92279\n",
            "[20]\tvalidation_0-mlogloss:1.84043\tvalidation_1-mlogloss:1.90186\n",
            "[21]\tvalidation_0-mlogloss:1.82005\tvalidation_1-mlogloss:1.88192\n",
            "[22]\tvalidation_0-mlogloss:1.80079\tvalidation_1-mlogloss:1.86508\n",
            "[23]\tvalidation_0-mlogloss:1.78271\tvalidation_1-mlogloss:1.84722\n",
            "[24]\tvalidation_0-mlogloss:1.76551\tvalidation_1-mlogloss:1.83134\n",
            "[25]\tvalidation_0-mlogloss:1.74933\tvalidation_1-mlogloss:1.81654\n",
            "[26]\tvalidation_0-mlogloss:1.73337\tvalidation_1-mlogloss:1.80229\n",
            "[27]\tvalidation_0-mlogloss:1.71871\tvalidation_1-mlogloss:1.78882\n",
            "[28]\tvalidation_0-mlogloss:1.70484\tvalidation_1-mlogloss:1.77545\n",
            "[29]\tvalidation_0-mlogloss:1.69161\tvalidation_1-mlogloss:1.76272\n",
            "[30]\tvalidation_0-mlogloss:1.67887\tvalidation_1-mlogloss:1.75089\n",
            "[31]\tvalidation_0-mlogloss:1.66676\tvalidation_1-mlogloss:1.73934\n",
            "[32]\tvalidation_0-mlogloss:1.65504\tvalidation_1-mlogloss:1.72950\n",
            "[33]\tvalidation_0-mlogloss:1.64392\tvalidation_1-mlogloss:1.72000\n",
            "[34]\tvalidation_0-mlogloss:1.63333\tvalidation_1-mlogloss:1.71053\n",
            "[35]\tvalidation_0-mlogloss:1.62305\tvalidation_1-mlogloss:1.70087\n",
            "[36]\tvalidation_0-mlogloss:1.61317\tvalidation_1-mlogloss:1.69171\n",
            "[37]\tvalidation_0-mlogloss:1.60384\tvalidation_1-mlogloss:1.68298\n",
            "[38]\tvalidation_0-mlogloss:1.59462\tvalidation_1-mlogloss:1.67393\n",
            "[39]\tvalidation_0-mlogloss:1.58577\tvalidation_1-mlogloss:1.66547\n",
            "[40]\tvalidation_0-mlogloss:1.57723\tvalidation_1-mlogloss:1.65711\n",
            "[41]\tvalidation_0-mlogloss:1.56908\tvalidation_1-mlogloss:1.64931\n",
            "[42]\tvalidation_0-mlogloss:1.56116\tvalidation_1-mlogloss:1.64242\n",
            "[43]\tvalidation_0-mlogloss:1.55353\tvalidation_1-mlogloss:1.63578\n",
            "[44]\tvalidation_0-mlogloss:1.54618\tvalidation_1-mlogloss:1.63045\n",
            "[45]\tvalidation_0-mlogloss:1.53893\tvalidation_1-mlogloss:1.62412\n",
            "[46]\tvalidation_0-mlogloss:1.53198\tvalidation_1-mlogloss:1.61772\n",
            "[47]\tvalidation_0-mlogloss:1.52514\tvalidation_1-mlogloss:1.61245\n",
            "[48]\tvalidation_0-mlogloss:1.51861\tvalidation_1-mlogloss:1.60660\n",
            "[49]\tvalidation_0-mlogloss:1.51232\tvalidation_1-mlogloss:1.60071\n",
            "[50]\tvalidation_0-mlogloss:1.50617\tvalidation_1-mlogloss:1.59605\n",
            "[51]\tvalidation_0-mlogloss:1.50019\tvalidation_1-mlogloss:1.59052\n",
            "[52]\tvalidation_0-mlogloss:1.49440\tvalidation_1-mlogloss:1.58544\n",
            "[53]\tvalidation_0-mlogloss:1.48869\tvalidation_1-mlogloss:1.58069\n",
            "[54]\tvalidation_0-mlogloss:1.48322\tvalidation_1-mlogloss:1.57589\n",
            "[55]\tvalidation_0-mlogloss:1.47789\tvalidation_1-mlogloss:1.57131\n",
            "[56]\tvalidation_0-mlogloss:1.47270\tvalidation_1-mlogloss:1.56706\n",
            "[57]\tvalidation_0-mlogloss:1.46761\tvalidation_1-mlogloss:1.56265\n",
            "[58]\tvalidation_0-mlogloss:1.46267\tvalidation_1-mlogloss:1.55808\n",
            "[59]\tvalidation_0-mlogloss:1.45791\tvalidation_1-mlogloss:1.55395\n",
            "[60]\tvalidation_0-mlogloss:1.45326\tvalidation_1-mlogloss:1.54992\n",
            "[61]\tvalidation_0-mlogloss:1.44865\tvalidation_1-mlogloss:1.54663\n",
            "[62]\tvalidation_0-mlogloss:1.44419\tvalidation_1-mlogloss:1.54313\n",
            "[63]\tvalidation_0-mlogloss:1.43981\tvalidation_1-mlogloss:1.53945\n",
            "[64]\tvalidation_0-mlogloss:1.43543\tvalidation_1-mlogloss:1.53634\n",
            "[65]\tvalidation_0-mlogloss:1.43122\tvalidation_1-mlogloss:1.53290\n",
            "[66]\tvalidation_0-mlogloss:1.42705\tvalidation_1-mlogloss:1.52948\n",
            "[67]\tvalidation_0-mlogloss:1.42290\tvalidation_1-mlogloss:1.52592\n",
            "[68]\tvalidation_0-mlogloss:1.41891\tvalidation_1-mlogloss:1.52283\n",
            "[69]\tvalidation_0-mlogloss:1.41505\tvalidation_1-mlogloss:1.51977\n",
            "[70]\tvalidation_0-mlogloss:1.41131\tvalidation_1-mlogloss:1.51635\n",
            "[71]\tvalidation_0-mlogloss:1.40750\tvalidation_1-mlogloss:1.51381\n",
            "[72]\tvalidation_0-mlogloss:1.40374\tvalidation_1-mlogloss:1.51097\n",
            "[73]\tvalidation_0-mlogloss:1.39988\tvalidation_1-mlogloss:1.50818\n",
            "[74]\tvalidation_0-mlogloss:1.39627\tvalidation_1-mlogloss:1.50482\n",
            "[75]\tvalidation_0-mlogloss:1.39269\tvalidation_1-mlogloss:1.50171\n",
            "[76]\tvalidation_0-mlogloss:1.38904\tvalidation_1-mlogloss:1.49914\n",
            "[77]\tvalidation_0-mlogloss:1.38561\tvalidation_1-mlogloss:1.49638\n",
            "[78]\tvalidation_0-mlogloss:1.38212\tvalidation_1-mlogloss:1.49381\n",
            "[79]\tvalidation_0-mlogloss:1.37881\tvalidation_1-mlogloss:1.49110\n",
            "[80]\tvalidation_0-mlogloss:1.37561\tvalidation_1-mlogloss:1.48892\n",
            "[81]\tvalidation_0-mlogloss:1.37235\tvalidation_1-mlogloss:1.48707\n",
            "[82]\tvalidation_0-mlogloss:1.36917\tvalidation_1-mlogloss:1.48459\n",
            "[83]\tvalidation_0-mlogloss:1.36601\tvalidation_1-mlogloss:1.48195\n",
            "[84]\tvalidation_0-mlogloss:1.36284\tvalidation_1-mlogloss:1.47956\n",
            "[85]\tvalidation_0-mlogloss:1.35969\tvalidation_1-mlogloss:1.47728\n",
            "[86]\tvalidation_0-mlogloss:1.35666\tvalidation_1-mlogloss:1.47535\n",
            "[87]\tvalidation_0-mlogloss:1.35373\tvalidation_1-mlogloss:1.47316\n",
            "[88]\tvalidation_0-mlogloss:1.35074\tvalidation_1-mlogloss:1.47145\n",
            "[89]\tvalidation_0-mlogloss:1.34771\tvalidation_1-mlogloss:1.46882\n",
            "[90]\tvalidation_0-mlogloss:1.34485\tvalidation_1-mlogloss:1.46671\n",
            "[91]\tvalidation_0-mlogloss:1.34202\tvalidation_1-mlogloss:1.46435\n",
            "[92]\tvalidation_0-mlogloss:1.33918\tvalidation_1-mlogloss:1.46207\n",
            "[93]\tvalidation_0-mlogloss:1.33641\tvalidation_1-mlogloss:1.46003\n",
            "[94]\tvalidation_0-mlogloss:1.33367\tvalidation_1-mlogloss:1.45782\n",
            "[95]\tvalidation_0-mlogloss:1.33099\tvalidation_1-mlogloss:1.45555\n",
            "[96]\tvalidation_0-mlogloss:1.32825\tvalidation_1-mlogloss:1.45388\n",
            "[97]\tvalidation_0-mlogloss:1.32554\tvalidation_1-mlogloss:1.45178\n",
            "[98]\tvalidation_0-mlogloss:1.32279\tvalidation_1-mlogloss:1.44997\n",
            "[99]\tvalidation_0-mlogloss:1.32019\tvalidation_1-mlogloss:1.44828\n",
            "Validation Accuracy: 66.00%\n"
          ]
        }
      ],
      "source": [
        "# Make sure you comment your code clearly and you may refer to these comments in the part 2.2\n",
        "# TODO\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "pca = PCA(n_components=1000)  # Keep 500 components\n",
        "\n",
        "train_vectors_reduced = train_vectors.numpy()\n",
        "valid_vectors_reduced = valid_vectors.numpy()\n",
        "test_vectors_reduced = test_vectors.numpy()\n",
        "\n",
        "\n",
        "\n",
        "train_labels = train_labels.long()\n",
        "valid_labels = valid_labels.long()\n",
        "\n",
        "\n",
        "eval_set = [(train_vectors_reduced,train_labels), (valid_vectors_reduced, valid_labels)]\n",
        "\n",
        "# Train XGBoost model\n",
        "model = xgb.XGBClassifier(\n",
        "    max_depth=4,\n",
        "    learning_rate=0.1,\n",
        "    n_estimators=100,\n",
        "    objective=\"multi:softmax\",\n",
        "    num_class=28  # Number of classes in the dataset\n",
        ")\n",
        "model.fit(train_vectors_reduced, train_labels,eval_set = eval_set, verbose =True)\n",
        "\n",
        "# Evaluate on validation set\n",
        "val_predictions = model.predict(valid_vectors_reduced)\n",
        "val_accuracy = accuracy_score(valid_labels.numpy(), val_predictions)\n",
        "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Predict on test set\n",
        "test_predictions = model.predict(test_vectors_reduced)\n",
        "\n",
        "# Save predictions\n",
        "predictions_df = pd.DataFrame({'id': test['id'], 'label': test_predictions})\n",
        "predictions_df.to_csv('submission2.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvbYo2dQXjpC"
      },
      "source": [
        "<h3>1.4 Explanation in Words:</h3><p>\n",
        "    You need to answer the following questions in the markdown cell after this cell:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbeIW5i7XjpC"
      },
      "source": [
        "1.4.1 How did you formulate the learning problem?\n",
        "\n",
        "To begin with, we knew we needed to convert the text from the sentences into something that a machine learning algorithm can use. We also knew we needed to come up with a way for our model to select the best category of the 28 to choose from. We decided the best way to go about this would be to have our model give a predicted value between 1 and 28 that corresponds to the 28 possible classes. This predicted class can be determined by a probability distribution across all of the categories and select the most likely label for one, or through other rules for comparing feature vector similarities. We decided the best way to evaluate our models performance, and improve upon it is by looking at the models accuracy to get an idea for how well the model is making predictions.\n",
        "\n",
        "\n",
        "After converting the text into a vector, we needed to learn a model on the vectors to correctly predict patterns in the vectors and their emotion classification.\n",
        "\n",
        "\n",
        "\n",
        "1.4.2 Which two learning methods from class did you choose and why did you made the choices?\n",
        "\n",
        "We chose to use a feed forward neural network with a leaky relu activation function that outputs a vector of dim = 28 for each categorization. The value of this represents the likelihood of the respective category according to the model with the highest probability selected as the model’s output. For our second model we used decision trees with gradient boosting to classify text. We chose to use this learning method because we felt that decision trees work well generally for categorizing text into multiple “buckets”, and we felt boosting would allow us to avoid some of the pitfalls of trees like overfitting, especially since our feature vectors were large.\n",
        "\n",
        "\n",
        "1.4.3 How did you do the model selection?\n",
        "\n",
        "\n",
        "In order to learn a model on natural language, we created a bag of word vectorization functions to convert text into numbers. Bag of words provided a good balance of easy computation and easy implementation while being fairly effective. We created a set called vocab to store all of the unique words. In order to cut down on features and unimportant words, I used the spacy library to filter out “stop words” such as “and” or “to” that do not contribute to sentiment as much as other words. Additionally, we converted verbs such as “running” into “run” to further cut down on unnecessary features.\n",
        "\n",
        "\n",
        "We went about model selection for the feed forward network by hyperparameter tuning. We tried adjusting the amount of layers, and we found by including additional hidden layers the model was overfitting which was evident by a near zero training loss, but a validation loss larger than 2.0. However, we also found that removing the third layer decreased perform\tance. Hence, we decreased the amount of epochs as well as included regularization methods in the optimizer and added dropout, keeping the improved performance while reducing overfitting.\n",
        "\n",
        "\n",
        "For the gradient boosted decision trees we tried using PCA adt first to truncate the amount of features but found that the model performed best when trained on the full feature set. Additionally we experimented with the tree height finding that a smaller height actually provided slightly better performance.We went from a tree with a height of 6 to a height of 4.\n",
        "\n",
        "\n",
        "\n",
        "1.4.4 Does the test performance reach the first baseline \"Tiny Piney\"? (Please include a screenshot of Kaggle Submission) [link text](https://)\n",
        "\n",
        "Yes, our basic solution exceeded Tiny Piney\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbqZsgIrXjpC"
      },
      "source": [
        "<h2>Part 2: Be creative!</h2><p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaoVZ5qTXjpC"
      },
      "source": [
        "<h3>2.1 Open-ended Code:</h3><p>\n",
        "You may follow the steps in part 1 again but making innovative changes like using new training algorithms, etc. Make sure you explain everything clearly in part 2.2. Note that beating \"Zero Hero\" is only a small portion of this part. Any creative ideas will receive most points as long as they are reasonable and clearly explained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "nVtXXkukXjpC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('train.csv')\n",
        "\n",
        "test_data = pd.read_csv('test.csv')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "oHont7KdY-83",
        "outputId": "6b697ffe-b880-4d0f-d244-b6930b5d487d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',num_labels = 28).to('cpu')\n",
        "\n",
        "inputs = tokenizer(data[\"text\"].to_list(), padding=True, truncation=True, return_tensors='pt')\n",
        "labels = torch.tensor(data[\"label\"])\n",
        "\n",
        "test_inputs = tokenizer(test_data[\"text\"].to_list(), padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item['labels'] = self.labels[idx]\n",
        "        return item\n",
        "dataset = TextDataset(inputs, labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Ukk1VlPnZFa6"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "\n",
        "def compute_metrics(pred):\n",
        "  labels = pred.label_ids\n",
        "  preds = pred.predictions.argmax(-1)\n",
        "  #score = accuracy_score(labels, preds)\n",
        "  score = f1_score(labels, preds, average='weighted')\n",
        "  acc = accuracy_score(labels, preds)\n",
        "  return {\n",
        "      'f1': score,\n",
        "      'accuracy': acc,\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691
        },
        "id": "mrmP47bpZHHt",
        "outputId": "8359a01f-e0c7-42b9-b4b0-eb6b4ec28b23"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 01:05, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.709800</td>\n",
              "      <td>1.861923</td>\n",
              "      <td>0.365795</td>\n",
              "      <td>0.495000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.623700</td>\n",
              "      <td>1.317019</td>\n",
              "      <td>0.565912</td>\n",
              "      <td>0.633000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.147300</td>\n",
              "      <td>1.099909</td>\n",
              "      <td>0.652605</td>\n",
              "      <td>0.696500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.992200</td>\n",
              "      <td>1.020955</td>\n",
              "      <td>0.693295</td>\n",
              "      <td>0.723000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.006500</td>\n",
              "      <td>0.938475</td>\n",
              "      <td>0.719269</td>\n",
              "      <td>0.743500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.826400</td>\n",
              "      <td>0.906927</td>\n",
              "      <td>0.724391</td>\n",
              "      <td>0.744000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.767800</td>\n",
              "      <td>0.920690</td>\n",
              "      <td>0.728530</td>\n",
              "      <td>0.752500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.834300</td>\n",
              "      <td>0.872855</td>\n",
              "      <td>0.727389</td>\n",
              "      <td>0.753500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.729900</td>\n",
              "      <td>0.844276</td>\n",
              "      <td>0.739570</td>\n",
              "      <td>0.756500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.717700</td>\n",
              "      <td>0.831046</td>\n",
              "      <td>0.739738</td>\n",
              "      <td>0.760500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.634500</td>\n",
              "      <td>0.836272</td>\n",
              "      <td>0.737852</td>\n",
              "      <td>0.760000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.548900</td>\n",
              "      <td>0.824973</td>\n",
              "      <td>0.743118</td>\n",
              "      <td>0.765500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.596200</td>\n",
              "      <td>0.819642</td>\n",
              "      <td>0.740468</td>\n",
              "      <td>0.762500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.563500</td>\n",
              "      <td>0.818634</td>\n",
              "      <td>0.743192</td>\n",
              "      <td>0.764500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.581500</td>\n",
              "      <td>0.815658</td>\n",
              "      <td>0.746244</td>\n",
              "      <td>0.766500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 00:01]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.8156575560569763,\n",
              " 'eval_f1': 0.7462436776875906,\n",
              " 'eval_accuracy': 0.7665,\n",
              " 'eval_runtime': 1.1076,\n",
              " 'eval_samples_per_second': 1805.708,\n",
              " 'eval_steps_per_second': 90.285,\n",
              " 'epoch': 3.0}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=20,\n",
        "    learning_rate=5e-5,\n",
        "    warmup_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        "    eval_strategy='steps',\n",
        "    output_dir='./results',\n",
        "    run_name='my_experiment',\n",
        "    report_to='none',\n",
        "    seed = 42,\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics = compute_metrics\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "nYCF3rWhyYzl",
        "outputId": "c989e923-dfc9-4cec-eea4-a1888c5c4818"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PredictionOutput(predictions=array([[-3.2994623e+00, -4.1913283e-01, -3.1433434e+00, ...,\n",
            "        -3.4308878e-01, -2.6346073e+00,  6.8985143e+00],\n",
            "       [-3.4019110e+00, -1.0683243e+00, -2.2352924e+00, ...,\n",
            "        -5.0666088e-01, -3.2107604e+00, -4.8267762e-03],\n",
            "       [-3.1755946e+00,  4.3650618e-01, -2.8848526e+00, ...,\n",
            "        -9.5433182e-01, -2.5741169e+00,  7.8771424e-01],\n",
            "       ...,\n",
            "       [-3.3728209e+00, -1.9593272e+00, -1.9619123e+00, ...,\n",
            "         2.9653367e-01, -2.8483663e+00, -8.4335697e-01],\n",
            "       [-2.2238469e+00,  8.2302198e+00, -2.1392069e+00, ...,\n",
            "        -1.7103633e+00, -3.0423563e+00,  6.4389908e-01],\n",
            "       [-3.4659858e+00,  1.3115994e+00, -2.6390691e+00, ...,\n",
            "        -1.9886749e+00, -3.1556966e+00,  6.4458926e-03]], dtype=float32), label_ids=None, metrics={'test_runtime': 34.8425, 'test_samples_per_second': 430.509, 'test_steps_per_second': 21.525})\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from datasets import Dataset\n",
        "test_dataset = Dataset.from_dict({\n",
        "    \"input_ids\": test_inputs[\"input_ids\"],\n",
        "    \"attention_mask\": test_inputs[\"attention_mask\"],\n",
        "\n",
        "})\n",
        "test_results = trainer.predict(test_dataset)\n",
        "print(test_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "GbnNOymX08vl",
        "outputId": "c7a8efb1-36af-4701-ae82-6d33a9165382"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          id  predictions\n",
            "0          0           27\n",
            "1          1           16\n",
            "2          2           21\n",
            "3          3           21\n",
            "4          4           21\n",
            "...      ...          ...\n",
            "14995  14995            9\n",
            "14996  14996            9\n",
            "14997  14997           12\n",
            "14998  14998            1\n",
            "14999  14999            4\n",
            "\n",
            "[15000 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "predictions = test_results.predictions\n",
        "predicted_classes = predictions.argmax(axis=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df = pd.DataFrame({'id': [id for id in range(len(test_data))], 'predictions': predicted_classes})\n",
        "\n",
        "\n",
        "df.to_csv('submission3.csv', index=False)\n",
        "\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lupUvIVXjpC"
      },
      "source": [
        "<h3>2.2 Explanation in Words:</h3><p>\n",
        "You need to answer the following questions in a markdown cell after this cell:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN8qZk9BXjpC"
      },
      "source": [
        "2.2.1 How much did you manage to improve performance on the test set? Did you beat \"Zero Hero\" in Kaggle? (Please include a screenshot of Kaggle Submission)\n",
        "\n",
        "2.2.2 Please explain in detail how you achieved this and what you did specifically and why you tried this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwB8gy94XjpC"
      },
      "source": [
        "<h2>Part 3: Kaggle Submission</h2><p>\n",
        "You need to generate a prediction CSV using the following cell from your trained model and submit the direct output of your code to Kaggle. The results should be presented in two columns in csv format: the first column is the data id (0-14999) and the second column includes the predictions for the test set. The first column must be named id and the second column must be named label (otherwise your submission will fail). A sample predication file can be downloaded from Kaggle for each problem.\n",
        "We provide how to save a csv file if you are running Notebook on Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSv-63aXXjpC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "id = range(15000)\n",
        "prediction = range(15000)\n",
        "submission = pd.DataFrame({'id': id, 'label': prediction})\n",
        "submission.to_csv('/kaggle/working/submission.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDezo1oUXjpD",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "\n",
        "# You may use pandas to generate a dataframe with country, date and your predictions first\n",
        "# and then use to_csv to generate a CSV file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXn9zcOrXjpD"
      },
      "source": [
        "<h2>Part 4: Resources and Literature Used</h2><p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDarCvjRXjpD"
      },
      "source": [
        "Please cite the papers and open resources you used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQs74bp6XjpD"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "databundleVersionId": 10240856,
          "sourceId": 88281,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30804,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
