{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqFXCHPhXjo8"
      },
      "source": [
        "<h3>Emotion Classification of Natural Language</h3>\n",
        "\n",
        "Team Members:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJE9a_I8Xjo9"
      },
      "source": [
        "<h3>Project Overview:</h3>\n",
        "\n",
        "<p>This project involves building and evaluating machine learning models to classify text by the emotion it conveys. The task is to predict one of 28 possible emotion classes from a given text. The notebook demonstrates various preprocessing steps, model training using both neural networks and gradient boosted decision trees, and generating predictions for submission.</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdXhJIIaXjo-"
      },
      "source": [
        "<h2>Part 0: Setup and Imports</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSFo0TKaXjo-"
      },
      "source": [
        "<h3>0.1 Import Libraries:</h3>\n",
        "<p>Import all the necessary packages for text processing and machine learning.</p>\n",
        "<p>Useful resources:</p>\n",
        "<ul>\n  <li><a href=\"https://scikit-learn.org/stable/tutorial/basic/tutorial.html\">scikit-learn tutorial</a></li>\n  <li><a href=\"https://pytorch.org/tutorials/\">PyTorch tutorials</a></li>\n  <li><a href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html\">pandas quickstart</a></li>\n</ul>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "execution": {
          "iopub.execute_input": "2024-12-09T15:12:09.391215Z",
          "iopub.status.busy": "2024-12-09T15:12:09.390742Z"
        },
        "id": "Z8BL_R9RXjo-",
        "outputId": "a34fab01-32d8-4172-b293-0f274b51bb47",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.15.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "# !pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "SvKvna-oXjo_",
        "outputId": "7a793d9e-d32b-475c-b531-757ab6e7eaea",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# !python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "4c3ZNHxXXjpA",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import spacy\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import Trainer, TrainingArguments\n",
        "# Additional code will follow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UU6TrGOXjpA"
      },
      "source": [
        "<h3>0.2 Accuracy Example:</h3>\n",
        "<p>Below is an example of using accuracy_score to measure performance.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "xGOn9Q3MXjpA",
        "outputId": "7e69b87a-d478-43b1-b6cb-0dea34fd41bf",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.42857142857142855"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "y_pred = [3, 2, 1, 0, 1, 2, 3]\n",
        "y_true = [0, 1, 2, 3, 1, 2, 3]\n",
        "accuracy_score(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1k6_AQ8XjpA"
      },
      "source": [
        "<h2>Part 1: Basic Modeling</h2>\n",
        "<p>This section loads the data, preprocesses the text, and builds two machine learning models.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pJ1yUV0XjpA"
      },
      "source": [
        "<h3>1.1 Load and Preprocess the Data:</h3>\n",
        "<p>Load the training and test datasets and extract the text and labels.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "tYR6B0fLXjpA",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Load training data\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "train_text = train[\"text\"]\n",
        "train_label = train[\"label\"]\n",
        "\n",
        "# Load test data\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "test_id = test[\"id\"]\n",
        "test_text = test[\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "N3fLn5XwXjpB",
        "outputId": "71afa0c8-3a4e-4b51-f093-c486cd9a39dd",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"train\",\n  \"rows\": 10000,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9936,\n        \"samples\": [\n          \"i don t know what i ve done to curry favor with this particular new dancer but the feeling of being lauded was more pleasant than taxing for a change\",\n          \"i remember him feeling discouraged\",\n          \"i am able to feel incomparability of life most precious resources to your affectionate touch that can bring warmth and awaken my cautious yet feeble heart together our bonds is able to release the troubles that bother us they will steadily sink to darkest depths of the ocean\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 0,\n        \"max\": 27,\n        \"num_unique_values\": 28,\n        \"samples\": [\n          17,\n          0,\n          16\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "train"
            },
            "text/html": [
              "<div id=\"df-b345403e-e47a-4be8-bcaf-906e793dd536\" class=\"colab-df-container\"> ... </div>"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Preview the training data\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ubTT4VQXjpB"
      },
      "source": [
        "<h3>1.2 Implement Two Training Algorithms:</h3>\n",
        "<p>This section demonstrates two approaches: a neural network using bag-of-words and a gradient boosted decision tree model.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiNRn2f-IrGi"
      },
      "source": [
        "<strong>Bag of Words Vectorization</strong>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8h7_c16OXjpB",
        "outputId": "5ed96fea-e34c-4edf-cfde-9e072876c09b",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        }
      ],
      "source": [
        "# Load spaCy model for text preprocessing\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Preprocess text: lemmatize and remove stop words\n",
        "cleaned = []\n",
        "for text in train['text']:\n",
        "    doc = nlp(text)\n",
        "    filtered = [word.lemma_ for word in doc if not word.is_stop]\n",
        "    cleaned.append(filtered)\n",
        "\n",
        "# Build vocabulary of unique words\n",
        "vocab = {}\n",
        "for sentence in cleaned:\n",
        "    for word in sentence:\n",
        "        if word not in vocab:\n",
        "            vocab[word] = len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "9c0LVTogXjpB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Function to vectorize text using bag-of-words representation\n",
        "def vectorize(text, vocab):\n",
        "  vector = np.zeros(len(vocab))\n",
        "  for word in text.lower().split():\n",
        "    if word in vocab:\n",
        "      vector[vocab[word]] += 1\n",
        "  return vector\n",
        "\n",
        "# Convert sentences to vectors and store corresponding labels\n",
        "vectors = []\n",
        "labels = []\n",
        "for text, label in zip(train['text'], train['label']):\n",
        "    vectors.append(vectorize(text, vocab))\n",
        "    labels.append(label)\n",
        "\n",
        "vectors = torch.tensor(np.array(vectors), dtype=torch.float32)\n",
        "labels = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "# Define a neural network model using bag-of-words input\n",
        "class BoWModel(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, output_size):\n",
        "        super(BoWModel, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(vocab_size, hidden_size)\n",
        "        self.fc2 = torch.nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = torch.nn.Linear(hidden_size, output_size)\n",
        "        self.bn1 = torch.nn.BatchNorm1d(hidden_size)\n",
        "        self.dropout = torch.nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        leaky = torch.nn.LeakyReLU(negative_slope=0.1)\n",
        "        x = leaky(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = leaky(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = BoWModel(len(vocab), 100, 28)\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_XARkHRXjpB"
      },
      "source": [
        "<h3>1.3 Train, Validate, and Select Model:</h3>\n",
        "<p>Split the data into training and validation sets and train the neural network.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-HB5Q8kZXjpB",
        "outputId": "2b224ca3-16c1-40e0-a213-970fc9cdea84",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "  Train Loss: 3.3998\n",
            "  Validation Loss: 3.3438\n",
            "  Validation Accuracy: 0.70%\n",
            "Epoch 26/200\n",
            "  Train Loss: 3.0209\n",
            "  Validation Loss: 3.3145\n",
            "  Validation Accuracy: 2.35%\n",
            "Epoch 51/200\n",
            "  Train Loss: 2.6353\n",
            "  Validation Loss: 3.1966\n",
            "  Validation Accuracy: 41.90%\n",
            "Epoch 76/200\n",
            "  Train Loss: 2.2344\n",
            "  Validation Loss: 2.7724\n",
            "  Validation Accuracy: 58.80%\n",
            "Epoch 101/200\n",
            "  Train Loss: 1.8446\n",
            "  Validation Loss: 2.2464\n",
            "  Validation Accuracy: 63.05%\n",
            "Epoch 126/200\n",
            "  Train Loss: 1.5118\n",
            "  Validation Loss: 1.9260\n",
            "  Validation Accuracy: 65.45%\n",
            "Epoch 151/200\n",
            "  Train Loss: 1.2496\n",
            "  Validation Loss: 1.6988\n",
            "  Validation Accuracy: 66.65%\n",
            "Epoch 176/200\n",
            "  Train Loss: 1.0507\n",
            "  Validation Loss: 1.5349\n",
            "  Validation Accuracy: 67.95%\n",
            "Epoch 200/200\n",
            "  Train Loss: 0.9174\n",
            "  Validation Loss: 1.4326\n",
            "  Validation Accuracy: 68.35%\n"
          ]
        }
      ],
      "source": [
        "# Split the data into training and validation sets (80% train, 20% validation)\n",
        "split = int(len(vectors) * 0.8)\n",
        "train_vectors = vectors[:split]\n",
        "train_labels = labels[:split]\n",
        "valid_vectors = vectors[split:]\n",
        "valid_labels = labels[split:]\n",
        "\n",
        "# Train the neural network model\n",
        "num_epochs = 200\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    train_preds = model(train_vectors)\n",
        "    train_loss = loss(train_preds, train_labels.long())\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        valid_preds = model(valid_vectors)\n",
        "        valid_loss = loss(valid_preds, valid_labels.long())\n",
        "        valid_preds_arg = torch.argmax(valid_preds, dim=1)\n",
        "        valid_accuracy = (valid_preds_arg == valid_labels.long()).sum().item() / valid_labels.size(0)\n",
        "\n",
        "    if epoch % 25 == 0 or epoch == num_epochs - 1:\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "        print(f\"  Train Loss: {train_loss.item():.4f}\")\n",
        "        print(f\"  Validation Loss: {valid_loss.item():.4f}\")\n",
        "        print(f\"  Validation Accuracy: {valid_accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "wSU4mdYJXjpC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Vectorize test set and generate predictions using the neural network model\n",
        "test_vectors = []\n",
        "for text in test['text']:\n",
        "    test_vectors.append(vectorize(text, vocab))\n",
        "test_vectors = torch.tensor(np.array(test_vectors), dtype=torch.float32)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_preds = model(test_vectors)\n",
        "    test_preds_arg = torch.argmax(test_preds, dim=1).numpy()\n",
        "\n",
        "predictions_df = pd.DataFrame({\n",
        "    'id': test['id'],\n",
        "    'label': test_preds_arg\n",
        "})\n",
        "\n",
        "predictions_df.to_csv('submission.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9flOOeImf5Mb",
        "outputId": "c30ec80d-028f-4d00-945a-c7f2b0af3a5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8000, 10035])\n"
          ]
        }
      ],
      "source": [
        "print(train_vectors.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAMPfs7tYjsD"
      },
      "source": [
        "Now we use a gradient boosted decision tree model as our second approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "n7UlOlMeYo5p",
        "outputId": "87ac7ff8-31ce-40a2-f074-518886e64298"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0]\tvalidation_0-mlogloss:3.05115\tvalidation_1-mlogloss:3.06361\n",
            "[1]\tvalidation_0-mlogloss:2.86536\tvalidation_1-mlogloss:2.88056\n",
            "[2]\tvalidation_0-mlogloss:2.72320\tvalidation_1-mlogloss:2.74551\n",
            "[3]\tvalidation_0-mlogloss:2.60764\tvalidation_1-mlogloss:2.63338\n",
            "[4]\tvalidation_0-mlogloss:2.51228\tvalidation_1-mlogloss:2.54337\n",
            "[5]\tvalidation_0-mlogloss:2.42938\tvalidation_1-mlogloss:2.46322\n",
            "[6]\tvalidation_0-mlogloss:2.35822\tvalidation_1-mlogloss:2.39629\n",
            "[7]\tvalidation_0-mlogloss:2.29524\tvalidation_1-mlogloss:2.33612\n",
            "[8]\tvalidation_0-mlogloss:2.23948\tvalidation_1-mlogloss:2.28323\n",
            "[9]\tvalidation_0-mlogloss:2.18893\tvalidation_1-mlogloss:2.23567\n",
            "[10]\tvalidation_0-mlogloss:2.14308\tvalidation_1-mlogloss:2.19147\n",
            "[11]\tvalidation_0-mlogloss:2.10106\tvalidation_1-mlogloss:2.14954\n",
            "[12]\tvalidation_0-mlogloss:2.06260\tvalidation_1-mlogloss:2.11459\n",
            "[13]\tvalidation_0-mlogloss:2.02728\tvalidation_1-mlogloss:2.08037\n",
            "[14]\tvalidation_0-mlogloss:1.99463\tvalidation_1-mlogloss:2.05003\n",
            "[15]\tvalidation_0-mlogloss:1.96439\tvalidation_1-mlogloss:2.02123\n",
            "[16]\tvalidation_0-mlogloss:1.93619\tvalidation_1-mlogloss:1.99489\n",
            "[17]\tvalidation_0-mlogloss:1.91001\tvalidation_1-mlogloss:1.97040\n",
            "[18]\tvalidation_0-mlogloss:1.88562\tvalidation_1-mlogloss:1.94669\n",
            "[19]\tvalidation_0-mlogloss:1.86222\tvalidation_1-mlogloss:1.92279\n",
            "[20]\tvalidation_0-mlogloss:1.84043\tvalidation_1-mlogloss:1.90186\n",
            "[21]\tvalidation_0-mlogloss:1.82005\tvalidation_1-mlogloss:1.88192\n",
            "[22]\tvalidation_0-mlogloss:1.80079\tvalidation_1-mlogloss:1.86508\n",
            "[23]\tvalidation_0-mlogloss:1.78271\tvalidation_1-mlogloss:1.84722\n",
            "[24]\tvalidation_0-mlogloss:1.76551\tvalidation_1-mlogloss:1.83134\n",
            "[25]\tvalidation_0-mlogloss:1.74933\tvalidation_1-mlogloss:1.81654\n",
            "[26]\tvalidation_0-mlogloss:1.73337\tvalidation_1-mlogloss:1.80229\n",
            "[27]\tvalidation_0-mlogloss:1.71871\tvalidation_1-mlogloss:1.78882\n",
            "[28]\tvalidation_0-mlogloss:1.70484\tvalidation_1-mlogloss:1.77545\n",
            "[29]\tvalidation_0-mlogloss:1.69161\tvalidation_1-mlogloss:1.76272\n",
            "[30]\tvalidation_0-mlogloss:1.67887\tvalidation_1-mlogloss:1.75089\n",
            "[31]\tvalidation_0-mlogloss:1.66676\tvalidation_1-mlogloss:1.73934\n",
            "[32]\tvalidation_0-mlogloss:1.65504\tvalidation_1-mlogloss:1.72950\n",
            "[33]\tvalidation_0-mlogloss:1.64392\tvalidation_1-mlogloss:1.72000\n",
            "[34]\tvalidation_0-mlogloss:1.63333\tvalidation_1-mlogloss:1.71053\n",
            "[35]\tvalidation_0-mlogloss:1.62305\tvalidation_1-mlogloss:1.70087\n",
            "[36]\tvalidation_0-mlogloss:1.61317\tvalidation_1-mlogloss:1.69171\n",
            "[37]\tvalidation_0-mlogloss:1.60384\tvalidation_1-mlogloss:1.68298\n",
            "[38]\tvalidation_0-mlogloss:1.59462\tvalidation_1-mlogloss:1.67393\n",
            "[39]\tvalidation_0-mlogloss:1.58577\tvalidation_1-mlogloss:1.66547\n",
            "[40]\tvalidation_0-mlogloss:1.57723\tvalidation_1-mlogloss:1.65711\n",
            "[41]\tvalidation_0-mlogloss:1.56908\tvalidation_1-mlogloss:1.64931\n",
            "[42]\tvalidation_0-mlogloss:1.56116\tvalidation_1-mlogloss:1.64242\n",
            "[43]\tvalidation_0-mlogloss:1.55353\tvalidation_1-mlogloss:1.63578\n",
            "[44]\tvalidation_0-mlogloss:1.54618\tvalidation_1-mlogloss:1.63045\n",
            "[45]\tvalidation_0-mlogloss:1.53893\tvalidation_1-mlogloss:1.62412\n",
            "[46]\tvalidation_0-mlogloss:1.53198\tvalidation_1-mlogloss:1.61772\n",
            "[47]\tvalidation_0-mlogloss:1.52514\tvalidation_1-mlogloss:1.61245\n",
            "[48]\tvalidation_0-mlogloss:1.51861\tvalidation_1-mlogloss:1.60660\n",
            "[49]\tvalidation_0-mlogloss:1.51232\tvalidation_1-mlogloss:1.60071\n",
            "[50]\tvalidation_0-mlogloss:1.50617\tvalidation_1-mlogloss:1.59605\n",
            "[51]\tvalidation_0-mlogloss:1.50019\tvalidation_1-mlogloss:1.59052\n",
            "[52]\tvalidation_0-mlogloss:1.49440\tvalidation_1-mlogloss:1.58544\n",
            "[53]\tvalidation_0-mlogloss:1.48869\tvalidation_1-mlogloss:1.58069\n",
            "[54]\tvalidation_0-mlogloss:1.48322\tvalidation_1-mlogloss:1.57589\n",
            "[55]\tvalidation_0-mlogloss:1.47789\tvalidation_1-mlogloss:1.57131\n",
            "[56]\tvalidation_0-mlogloss:1.47270\tvalidation_1-mlogloss:1.56706\n",
            "[57]\tvalidation_0-mlogloss:1.46761\tvalidation_1-mlogloss:1.56265\n",
            "[58]\tvalidation_0-mlogloss:1.46267\tvalidation_1-mlogloss:1.55808\n",
            "[59]\tvalidation_0-mlogloss:1.45791\tvalidation_1-mlogloss:1.55395\n",
            "[60]\tvalidation_0-mlogloss:1.45326\tvalidation_1-mlogloss:1.54992\n",
            "[61]\tvalidation_0-mlogloss:1.44865\tvalidation_1-mlogloss:1.54663\n",
            "[62]\tvalidation_0-mlogloss:1.44419\tvalidation_1-mlogloss:1.54313\n",
            "[63]\tvalidation_0-mlogloss:1.43981\tvalidation_1-mlogloss:1.53945\n",
            "[64]\tvalidation_0-mlogloss:1.43543\tvalidation_1-mlogloss:1.53634\n",
            "[65]\tvalidation_0-mlogloss:1.43122\tvalidation_1-mlogloss:1.53290\n",
            "[66]\tvalidation_0-mlogloss:1.42705\tvalidation_1-mlogloss:1.52948\n",
            "[67]\tvalidation_0-mlogloss:1.42290\tvalidation_1-mlogloss:1.52592\n",
            "[68]\tvalidation_0-mlogloss:1.41891\tvalidation_1-mlogloss:1.52283\n",
            "[69]\tvalidation_0-mlogloss:1.41505\tvalidation_1-mlogloss:1.51977\n",
            "[70]\tvalidation_0-mlogloss:1.41131\tvalidation_1-mlogloss:1.51635\n",
            "[71]\tvalidation_0-mlogloss:1.40750\tvalidation_1-mlogloss:1.51381\n",
            "[72]\tvalidation_0-mlogloss:1.40374\tvalidation_1-mlogloss:1.51097\n",
            "[73]\tvalidation_0-mlogloss:1.39988\tvalidation_1-mlogloss:1.50818\n",
            "[74]\tvalidation_0-mlogloss:1.39627\tvalidation_1-mlogloss:1.50482\n",
            "[75]\tvalidation_0-mlogloss:1.39269\tvalidation_1-mlogloss:1.50171\n",
            "[76]\tvalidation_0-mlogloss:1.38904\tvalidation_1-mlogloss:1.49914\n",
            "[77]\tvalidation_0-mlogloss:1.38561\tvalidation_1-mlogloss:1.49638\n",
            "[78]\tvalidation_0-mlogloss:1.38212\tvalidation_1-mlogloss:1.49381\n",
            "[79]\tvalidation_0-mlogloss:1.37881\tvalidation_1-mlogloss:1.49110\n",
            "[80]\tvalidation_0-mlogloss:1.37561\tvalidation_1-mlogloss:1.48892\n",
            "[81]\tvalidation_0-mlogloss:1.37235\tvalidation_1-mlogloss:1.48707\n",
            "[82]\tvalidation_0-mlogloss:1.36917\tvalidation_1-mlogloss:1.48459\n",
            "[83]\tvalidation_0-mlogloss:1.36601\tvalidation_1-mlogloss:1.48195\n",
            "[84]\tvalidation_0-mlogloss:1.36284\tvalidation_1-mlogloss:1.47956\n",
            "[85]\tvalidation_0-mlogloss:1.35969\tvalidation_1-mlogloss:1.47728\n",
            "[86]\tvalidation_0-mlogloss:1.35666\tvalidation_1-mlogloss:1.47535\n",
            "[87]\tvalidation_0-mlogloss:1.35373\tvalidation_1-mlogloss:1.47316\n",
            "[88]\tvalidation_0-mlogloss:1.35074\tvalidation_1-mlogloss:1.47145\n",
            "[89]\tvalidation_0-mlogloss:1.34771\tvalidation_1-mlogloss:1.46882\n",
            "[90]\tvalidation_0-mlogloss:1.34485\tvalidation_1-mlogloss:1.46671\n",
            "[91]\tvalidation_0-mlogloss:1.34202\tvalidation_1-mlogloss:1.46435\n",
            "[92]\tvalidation_0-mlogloss:1.33918\tvalidation_1-mlogloss:1.46207\n",
            "[93]\tvalidation_0-mlogloss:1.33641\tvalidation_1-mlogloss:1.46003\n",
            "[94]\tvalidation_0-mlogloss:1.33367\tvalidation_1-mlogloss:1.45782\n",
            "[95]\tvalidation_0-mlogloss:1.33099\tvalidation_1-mlogloss:1.45555\n",
            "[96]\tvalidation_0-mlogloss:1.32825\tvalidation_1-mlogloss:1.45388\n",
            "[97]\tvalidation_0-mlogloss:1.32554\tvalidation_1-mlogloss:1.45178\n",
            "[98]\tvalidation_0-mlogloss:1.32279\tvalidation_1-mlogloss:1.44997\n",
            "[99]\tvalidation_0-mlogloss:1.32019\tvalidation_1-mlogloss:1.44828\n",
            "Validation Accuracy: 66.00%\n"
          ]
        }
      ],
      "source": [
        "# Train and evaluate the gradient boosted decision tree model\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=1000)\n",
        "\n",
        "train_vectors_reduced = train_vectors.numpy()\n",
        "valid_vectors_reduced = valid_vectors.numpy()\n",
        "test_vectors_reduced = test_vectors.numpy()\n",
        "\n",
        "train_labels = train_labels.long()\n",
        "valid_labels = valid_labels.long()\n",
        "\n",
        "eval_set = [(train_vectors_reduced, train_labels), (valid_vectors_reduced, valid_labels)]\n",
        "\n",
        "model = xgb.XGBClassifier(\n",
        "    max_depth=4,\n",
        "    learning_rate=0.1,\n",
        "    n_estimators=100,\n",
        "    objective=\"multi:softmax\",\n",
        "    num_class=28\n",
        ")\n",
        "model.fit(train_vectors_reduced, train_labels, eval_set=eval_set, verbose=True)\n",
        "\n",
        "val_predictions = model.predict(valid_vectors_reduced)\n",
        "val_accuracy = accuracy_score(valid_labels.numpy(), val_predictions)\n",
        "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
        "\n",
        "test_predictions = model.predict(test_vectors_reduced)\n",
        "\n",
        "predictions_df = pd.DataFrame({'id': test['id'], 'label': test_predictions})\n",
        "predictions_df.to_csv('submission2.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvbYo2dQXjpC"
      },
      "source": [
        "<h3>1.4 Summary:</h3>\n",
        "<p>Summarize your approach, model design, and any performance insights here.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbqZsgIrXjpC"
      },
      "source": [
        "<h2>Part 2: Advanced Modeling and Experimentation</h2>\n",
        "<p>Explore additional techniques and innovative changes. Use new training algorithms or preprocessing methods as desired.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaoVZ5qTXjpC"
      },
      "source": [
        "<h3>2.1 Additional Experimentation:</h3>\n",
        "<p>Load the data and explore alternative modeling approaches.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "nVtXXkukXjpC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('train.csv')\n",
        "test_data = pd.read_csv('test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "oHont7KdY-83",
        "outputId": "6b697ffe-b880-4d0f-d244-b6930b5d487d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a downstream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=28).to('cpu')\n",
        "\n",
        "inputs = tokenizer(data[\"text\"].to_list(), padding=True, truncation=True, return_tensors='pt')\n",
        "labels = torch.tensor(data[\"label\"])\n",
        "\n",
        "test_inputs = tokenizer(test_data[\"text\"].to_list(), padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item['labels'] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "dataset = TextDataset(inputs, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Ukk1VlPnZFa6"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "def compute_metrics(pred):\n",
        "  labels = pred.label_ids\n",
        "  preds = pred.predictions.argmax(-1)\n",
        "  score = f1_score(labels, preds, average='weighted')\n",
        "  acc = accuracy_score(labels, preds)\n",
        "  return {\n",
        "      'f1': score,\n",
        "      'accuracy': acc\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691
        },
        "id": "mrmP47bpZHHt",
        "outputId": "8359a01f-e0c7-42b9-b4b0-eb6b4ec28b23"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n  <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n  [1500/1500 01:05, Epoch 3/3]\n</div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>2.709800</td>\n      <td>1.861923</td>\n      <td>0.365795</td>\n      <td>0.495000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.623700</td>\n      <td>1.317019</td>\n      <td>0.565912</td>\n      <td>0.633000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.147300</td>\n      <td>1.099909</td>\n      <td>0.652605</td>\n      <td>0.696500</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.992200</td>\n      <td>1.020955</td>\n      <td>0.693295</td>\n      <td>0.723000</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.006500</td>\n      <td>0.938475</td>\n      <td>0.719269</td>\n      <td>0.743500</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.826400</td>\n      <td>0.906927</td>\n      <td>0.724391</td>\n      <td>0.744000</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.767800</td>\n      <td>0.920690</td>\n      <td>0.728530</td>\n      <td>0.752500</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.834300</td>\n      <td>0.872855</td>\n      <td>0.727389</td>\n      <td>0.753500</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.729900</td>\n      <td>0.844276</td>\n      <td>0.739570</td>\n      <td>0.756500</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.717700</td>\n      <td>0.831046</td>\n      <td>0.739738</td>\n      <td>0.760500</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.634500</td>\n      <td>0.836272</td>\n      <td>0.737852</td>\n      <td>0.760000</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.548900</td>\n      <td>0.824973</td>\n      <td>0.743118</td>\n      <td>0.765500</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.596200</td>\n      <td>0.819642</td>\n      <td>0.740468</td>\n      <td>0.762500</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.563500</td>\n      <td>0.818634</td>\n      <td>0.743192</td>\n      <td>0.764500</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.581500</td>\n      <td>0.815658</td>\n      <td>0.746244</td>\n      <td>0.766500</td>\n    </tr>\n  </tbody>\n</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.8156575560569763,\n 'eval_f1': 0.7462436776875906,\n 'eval_accuracy': 0.7665,\n 'eval_runtime': 1.1076,\n 'eval_samples_per_second': 1805.708,\n 'eval_steps_per_second': 90.285,\n 'epoch': 3.0}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=20,\n",
        "    learning_rate=5e-5,\n",
        "    warmup_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        "    eval_strategy='steps',\n",
        "    output_dir='./results',\n",
        "    run_name='my_experiment',\n",
        "    report_to='none',\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "nYCF3rWhyYzl",
        "outputId": "c989e923-dfc9-4cec-eea4-a1888c5c4818"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PredictionOutput(predictions=array([[-3.2994623e+00, -4.1913283e-01, -3.1433434e+00, ...,\n",
            "        -3.4308878e-01, -2.6346073e+00,  6.8985143e+00],\n",
            "       [-3.4019110e+00, -1.0683243e+00, -2.2352924e+00, ...,\n",
            "        -5.0666088e-01, -3.2107604e+00, -4.8267762e-03],\n",
            "       [-3.1755946e+00,  4.3650618e-01, -2.8848526e+00, ...,\n",
            "        -9.5433182e-01, -2.5741169e+00,  7.8771424e-01],\n",
            "       ...,\n",
            "       [-3.3728209e+00, -1.9593272e+00, -1.9619123e+00, ...,\n",
            "         2.9653367e-01, -2.8483663e+00, -8.4335697e-01],\n",
            "       [-2.2238469e+00,  8.2302198e+00, -2.1392069e+00, ...,\n",
            "        -1.7103633e+00, -3.0423563e+00,  6.4389908e-01],\n",
            "       [-3.4659858e+00,  1.3115994e+00, -2.6390691e+00, ...,\n",
            "        -1.9886749e+00, -3.1556966e+00,  6.4458926e-03]], dtype=float32), label_ids=None, metrics={'test_runtime': 34.8425, 'test_samples_per_second': 430.509, 'test_steps_per_second': 21.525})\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "test_dataset = Dataset.from_dict({\n",
        "    \"input_ids\": test_inputs[\"input_ids\"],\n",
        "    \"attention_mask\": test_inputs[\"attention_mask\"]\n",
        "})\n",
        "test_results = trainer.predict(test_dataset)\n",
        "print(test_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "GbnNOymX08vl",
        "outputId": "c7a8efb1-36af-4701-ae82-6d33a9165382"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          id  predictions\n",
            "0          0           27\n",
            "1          1           16\n",
            "2          2           21\n",
            "3          3           21\n",
            "4          4           21\n",
            "...      ...          ...\n",
            "14995  14995            9\n",
            "14996  14996            9\n",
            "14997  14997           12\n",
            "14998  14998            1\n",
            "14999  14999            4\n",
            "\n",
            "[15000 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "predictions = test_results.predictions\n",
        "predicted_classes = predictions.argmax(axis=1)\n",
        "\n",
        "df = pd.DataFrame({'id': list(range(len(test_data))), 'predictions': predicted_classes})\n",
        "\n",
        "df.to_csv('submission3.csv', index=False)\n",
        "\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lupUvIVXjpC"
      },
      "source": [
        "<h3>2.2 Project Insights:</h3>\n",
        "<p>Discuss performance improvements, challenges, and the results of your experiments.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwB8gy94XjpC"
      },
      "source": [
        "<h2>Part 3: Generating Final Predictions for Deployment</h2>\n",
        "<p>Generate a CSV file with two columns: 'id' and 'label' for deployment.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSv-63aXXjpC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "id = range(15000)\n",
        "prediction = range(15000)\n",
        "submission = pd.DataFrame({'id': id, 'label': prediction})\n",
        "submission.to_csv('/kaggle/working/submission.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDezo1oUXjpD",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Additional code to generate a CSV file from your predictions using pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXn9zcOrXjpD"
      },
      "source": [
        "<h2>Part 4: References and Resources</h2>\n",
        "<p>Cite any papers or online resources you used.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDarCvjRXjpD"
      },
      "source": [
        "Please list your references here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQs74bp6XjpD"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "databundleVersionId": 10240856,
          "sourceId": 88281,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30804,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
